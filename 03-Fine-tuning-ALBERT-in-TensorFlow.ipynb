{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyCon KR 2020_Fine-tuning ALBERT in TensorFlow (GPU).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9n5rU-Mgzb-",
        "colab_type": "text"
      },
      "source": [
        "# (GPU) Fine-tuning KB-ALBERT with `Transformers` TensorFlow ver.\n",
        "\n",
        "Hugging Face의 Transformers는 미세조정(fine-tuning)을 위해 필요한 API들을 간단하게 사용할 수 있는 형태로 제공합니다.\n",
        "기초적인 파이썬 프로그래밍과 기계학습 모델의 학습 방법만 알고있다면, 자연어처리(NLP)를 전공하지 않은 일반 사용자도 쉽게 최신 인공지능 언어모델을 불러와 사용할 수 있습니다.\n",
        "\n",
        "그리고 특정 목적을 위해 미세조정된 기계학습 모델을 새로운 데이터에서 쉽게 예측해 볼 수 있도록 Inference Pipeline도 API로 제공하고 있습니다. 그래서 누구나 쉽게 자신이 학습한 모델을 PyTorch나 TensorFlow의 naive programming을 하지 않아도 쉽게 모델을 학습하고 학습된 모델을 배포할 수 있습니다.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "## Objective\n",
        "- Hugging Face's Transformers를 활용한 fine-tuning\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 네이버 영화리뷰 감성분석 (Naver Movie Review Sentiment Analysis)\n",
        "\n",
        "Naver sentiment movie corpus([link](https://github.com/e9t/nsmc))는 한국어 영화 리뷰 데이터로 리뷰 내용의 긍정과 부정이 라벨링 된 데이터입니다. 15만 개의 학습 데이터와 5만 개의 테스트 데이터로 나누어져 있습니다. 140개 이하의 짧은 문장으로 되어 있고, 쉽게 접근하여 사용해볼 수 있는 데이터입니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "## Contents\n",
        "\n",
        "1. 데이터 준비 & 필요 소스코드 다운로드\n",
        "2. Tokenizer를 활용한 학습용 데이터셋 생성\n",
        "3. 학습 하이퍼파라미터 설정\n",
        "4. Head를 활용한 모델 Fine-tuning\n",
        "5. Inference를 위한 Pipeline 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lha6eFEg4EG",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 1. 데이터 및 필요 소스코드 등 준비\n",
        "\n",
        "예제를 수행하기 위해 필요한 데이터와 소스코드를 다운로드 해야 합니다.\n",
        "그리고 아쉽게도 KB-ALBERT는 내부 라이선스 이슈로 Transformers Model Hub에 업로드되어 있지 않아 별도의 과정을 통해 요청하신 후에 다운로드 받아 사용할 수 있습니다. 요청 방법은 [링크](https://github.com/KB-BANK-AI/KB-ALBERT-KO/kb-albert-char)를 참고해주세요.\n",
        "\n",
        "> 참고: 본 예제는 음절단위 모델을 기준으로 작성되었습니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "실행 내용\n",
        "1. Download source code for Custom Tokenizer\n",
        "2. Install Transformers library\n",
        "3. Download NSMC dataset\n",
        "\n",
        "**원활한 모델 학습을 위해 GPU 환경에서 테스트하시기를 권장드립니다.<br> 위 메뉴창에서 \"런타임\" > \"런타임 유형변경\" > \"하드웨어가속기 GPU 선택\" > \"저장\" 으로 환경을 변경하실 수 있습니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3-1gX88iQ0z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "8aa49641-0b5e-4dab-e7f7-7ba2765ee3bc"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Sep  3 03:52:28 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7UF8h6CRtO4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "73558504-9a3a-43bd-b521-17750e92fa26"
      },
      "source": [
        "# Download source codes\n",
        "!git clone https://github.com/sackoh/pycon-korea-2020-kb-albert.git\n",
        "%cd pycon-korea-2020-kb-albert/\n",
        "\n",
        "# Install transformers\n",
        "%pip install -q transformers\n",
        "\n",
        "# Download NSMC dataset\n",
        "!wget -q https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "!wget -q https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pycon-korea-2020-kb-albert'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 7 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (7/7), done.\n",
            "/content/pycon-korea-2020-kb-albert\n",
            "\u001b[K     |████████████████████████████████| 890kB 9.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 30.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 60.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 57.9MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL2nhE7Hg_Su",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Upload pretrained model and configuration files\n",
        "\n",
        "다운로드 받은 파일을 **Google Colab**에서 사용하기 위해서는 파일 업로드가 필요합니다. 아래의 코드를 실행해주세요. 아래를 실행하면 파일 업로드를 위한 팝업창이 열립니다. 다음의 4가지 파일들을 *반드시* 선택하여 업로드를 진행해주세요.\n",
        "\n",
        "- config.json\n",
        "- pytorch_model.bin\n",
        "- tokenizer_config.json\n",
        "- vocab.txt\n",
        "\n",
        "업로드에 다소 시간이 걸릴 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg9x1O1yhCkp",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "bb9fd473-1bb7-48e5-d7fd-2108d72bf4a6"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "%mkdir kb-albert-char\n",
        "%mv config.json pytorch_model.bin tokenizer_config.json vocab.txt ./kb-albert-char"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a023891b-392b-4039-b6ab-bd1970ce4230\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a023891b-392b-4039-b6ab-bd1970ce4230\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving config.json to config.json\n",
            "Saving pytorch_model.bin to pytorch_model.bin\n",
            "Saving tokenizer_config.json to tokenizer_config.json\n",
            "Saving vocab.txt to vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COTuKvjBhHGS",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Load train & test data\n",
        "\n",
        "NSMC 데이터를 불러옵니다. train data와 test data는 각각 리뷰 텍스트와 리뷰의 긍부정 라벨로 구성되어 있습니다.\n",
        "\n",
        "| text | label |\n",
        "| ---  | ---   |\n",
        "| 아 더빙.. 진짜 짜증나네요 목소리 | 0 |\n",
        "| 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나 | 1 |\n",
        "| 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ... | 1 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-jWgROKhJFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "def read_nsmc_data(file_path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as r:\n",
        "        reader = csv.reader(r, delimiter=\"\\t\")\n",
        "        next(reader, None)\n",
        "        for line in reader:\n",
        "            texts.append(line[1])\n",
        "            labels.append(int(line[2]))\n",
        "    return texts, labels\n",
        "\n",
        "data_dir = Path('./')\n",
        "train_texts, train_labels = read_nsmc_data(data_dir/'ratings_train.txt')\n",
        "test_texts, test_labels = read_nsmc_data(data_dir/'ratings_test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maC04OFmhLAa",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 2. Tokenizer를 활용한 학습용 데이터셋 생성\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1kVfMy2hK72",
        "colab_type": "text"
      },
      "source": [
        "## Custom Tokenizer 불러오기\n",
        "\n",
        "본 예제는 음절단위(Character-level) 모델을 사용하기 때문에 한국어를 위한 음절단위 Tokenizer를 사용해야 합니다. `Transformers`에서 공식 Tokenizer로 등록이 안됐기 때문에 이전 단계에서 다운로드한 소스코드에서 Custom Tokenizer를 불러옵니다.\n",
        "\n",
        "만약 공식 Hub에 등록된 토크나이저를 사용한다면 다음과 같이 불러오면 됩니다.\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38wBfVXhkqZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tokenization_kbalbert import KbAlbertCharTokenizer\n",
        "\n",
        "tokenizer = KbAlbertCharTokenizer.from_pretrained('kb-albert-char')\n",
        "\n",
        "# from transformers import BertTokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmmQZ3L9hTi5",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Tokenizer를 통해 Raw text를 전처리\n",
        "\n",
        "불러온 tokenizer를 활용해 input 데이터를 전처리합니다. 텍스트를 sparse indices 형태로 변환하는 것이 주요 처리 내용입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36JojfK2hVJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BmX_Fu3hVkG",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Tensorflow Dataset으로 학습용 데이터셋 생성\n",
        "\n",
        "전처리된 데이터를 텐서플로의의 `Dataset` 안의 `from_tensor_slice`로 만들어줍니다. `from_tensor_slices`와 관련된 자세한 내용은 [링크](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices)를 참조해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEyGHXYthYRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "   dict(train_encodings),\n",
        "   train_labels\n",
        "))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-6hes8xhaJ1",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## 3. 학습 하이퍼파라미터 설정\n",
        "\n",
        "`Transformers`에서 제공하는 `TrainingArguments`에 학습 하이퍼파라미터를 설정해줍니다.\n",
        "\n",
        "설정하지 않은 하이퍼파라미터들은 default 값을 사용합니다.\n",
        "\n",
        "하이퍼파라미터 설정을 위한 상세 내용은 [링크](https://huggingface.co/transformers/main_classes/trainer.html?highlight=trainingargument#transformers.TrainingArguments)를 참조해주시기 바랍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2tZKaBhdp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import TFTrainingArguments\n",
        "\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2.0,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=2000\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkDJyXvTheWl",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 4. Head를 활용한 모델 Fine-tuning\n",
        "\n",
        "네이버 영화리뷰 감성분석을 위해 텍스트(sequence of words)에서 긍부정(1/0)을 예측하는 모델을 학습합니다. Sequence의 분류문제(Classification) 미세조정을 위한 Head인 `XXXForSequenceClassification`을 불러옵니다. 본 예제는 ALBERT 언어모델을 사용하기 때문에 `AlbertForSequenceClassification` Head class를 불러옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ah7zNPhhVV",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning을 위한 Head에 사전학습한 언어모델 불러오기\n",
        "\n",
        "Head의 `from_pretrained` 메소드를 통해 사전학습한 KB-ALBERT 언어모델을 함께 불러옵니다. 이때 인자값으로 모델이 위치한 디렉토리(폴더) 경로를 넘겨줍니다.\n",
        "\n",
        "> Head는 Pretrained Language Model과 Output Layer로 구성되어 있습니다.\n",
        "\n",
        "미세조정을 통해 Language Model과 Output Layer의 weight가 함께 조정됩니다.\n",
        "\n",
        "사전학습된 언어모델은 pytorch 모델이기 때문에 `from_pretrained`로 불러올 때 `from_pt=True`로 설정해주어어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS0I_kFlhkVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0a7e5d93-9c2b-4066-ee72-6c61c7338727"
      },
      "source": [
        "from transformers import TFAlbertForSequenceClassification\n",
        "\n",
        "with training_args.strategy.scope():\n",
        "    model = TFAlbertForSequenceClassification.from_pretrained('kb-albert-char', from_pt=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertForSequenceClassification: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing TFAlbertForSequenceClassification from a TF 2.0 model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a TFBertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from a TF 2.0 model that you expect to be exactly identical (e.g. initializing a BertForSequenceClassification model from a TFBertForSequenceClassification model).\n",
            "Some weights or buffers of the PyTorch model TFAlbertForSequenceClassification were not initialized from the TF 2.0 model and are newly initialized: ['predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'sop_classifier.classifier.bias', 'sop_classifier.classifier.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSDFnHqohmw_",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## TFTrainer를 통해 fine-tuning 수행\n",
        "\n",
        "그 다음은 학습을 위해 `TFTrainer` class를 불러옵니다. Head와 학습 하이퍼파라미터 그리고 앞에서 생성한 학습용 데이터셋을 인자로 넘긴 후에 `train()` 메소드로 학습을 시작합니다.\n",
        "\n",
        "P100 기준으로 약 1시간 정도 소요됐습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM7qenhvhmdU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d189a024-47fa-4fd1-8daf-434e96ecd87b"
      },
      "source": [
        "%%time\n",
        "from transformers import TFTrainer\n",
        "        \n",
        "trainer = TFTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1h 5min 3s, sys: 43min 46s, total: 1h 48min 49s\n",
            "Wall time: 1h 11min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdthYZkxhqpY",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## (Optional) Test data에 fine-tuned 모델 성능 평가하기\n",
        "\n",
        "모델 성능 평가를 위한 test dataset과 custom function을 trainer에게 전달하면 평가 결과를 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsHIsj1IhrKl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e75076d0-f9ef-4d32-fd6a-15a46605e3aa"
      },
      "source": [
        "import numpy as np\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\n",
        "            'acc': (preds == p.label_ids).mean()\n",
        "        }\n",
        "trainer.compute_metrics = compute_metrics\n",
        "trainer.evaluate(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_acc': 0.882882343550447, 'eval_loss': 0.2924079212081402}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beF0Mbm6hull",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 5. Inference를 위한 Pipeline 생성\n",
        "\n",
        "`Transformers`는 미세조정된 모델에 raw text를 인풋으로 넣었을 때 데이터 전처리와 모델 추론 과정을 `pipeline` class를 통해 api 형태로 쉽게 개발할 수 있도록 하였습니다.\n",
        "\n",
        "pipeline에는 사전 정의된 task 유형과 미세조정된 모델, tokenizer 그리고 딥러닝 프레임워크 유형을 인자로 전달합니다. (PyTorch는 'pt', TensorFlow는 'tf')\n",
        "\n",
        "생성된 pipeline 인스턴스에 raw text를 넣어주면 예측 결과와 예측 결과에 대한 confidence 값이 반환됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVY5voVyhwIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "4e466d91-359e-4238-ce17-d7dae81d1ba7"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nsmc_classifier = pipeline('sentiment-analysis', \n",
        "                           model=model, tokenizer=tokenizer, framework='tf')\n",
        "id2label = {\"LABEL_0\": \"negative\", \"LABEL_1\": \"positive\"}\n",
        "\n",
        "reviews= [\n",
        "          \"생각한 것보다는 영화가 재미없었어\",\n",
        "          \"OO!\",\n",
        "          \"역시 재밌네ㅋㅋ 최고\"\n",
        "]\n",
        "\n",
        "results = nsmc_classifier(reviews)\n",
        "for idx, result in enumerate(results):\n",
        "    print(reviews[idx])\n",
        "    for k, v in result.items():\n",
        "        print(f\" >> {k} : {id2label[v] if k == 'label' else v}\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "생각한 것보다는 영화가 재미없었어\n",
            " >> label : negative\n",
            " >> score : 0.9952784180641174\n",
            "\n",
            "OO!\n",
            " >> label : negative\n",
            " >> score : 0.8353099822998047\n",
            "\n",
            "역시 재밌네ㅋㅋ 최고\n",
            " >> label : positive\n",
            " >> score : 0.9927066564559937\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56kUjj0Nhy8u",
        "colab_type": "text"
      },
      "source": [
        "pipeline이나 미세조정된 모델은 `save_pretrained`를 통해 지정된 경로에 저장할 수 있습니다. 다시 사용해야 할 때는 같은 경로명을 `from_pretrained`에 넣어주어 쉽게 불러올 수 있습니다."
      ]
    }
  ]
}