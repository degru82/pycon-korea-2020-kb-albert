{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyCon KR 2020_Fine-tuning ALBERT in TensorFlow (TPU).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "87731483deed4002bc48cd3e3565f542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b9178d1919be4bf4b6491d361e8fee14",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2ad038ca23754fb492707b82e483aa9a",
              "IPY_MODEL_4006510ea3e84015bc9352a4e9ebf4a1"
            ]
          }
        },
        "b9178d1919be4bf4b6491d361e8fee14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ad038ca23754fb492707b82e483aa9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8bc7710c270541ad92efbc42c154517e",
            "_dom_classes": [],
            "description": "Epoch: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_704a6ef7cd3b44329af7a8f6fce4c407"
          }
        },
        "4006510ea3e84015bc9352a4e9ebf4a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_16e4756ea567472ba427cca12d7657d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [3:40:45&lt;00:00, 6622.56s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3560cb73a7e8456c8477c86137cdf354"
          }
        },
        "8bc7710c270541ad92efbc42c154517e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "704a6ef7cd3b44329af7a8f6fce4c407": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16e4756ea567472ba427cca12d7657d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3560cb73a7e8456c8477c86137cdf354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afe6c2729a354c639108c5c92ee61438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_be52f7c4c8574e99aea19e845959a590",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_336850901a214d11a7c0d9ed1859046e",
              "IPY_MODEL_06c3e6a3725a479082db75c038bcef53"
            ]
          }
        },
        "be52f7c4c8574e99aea19e845959a590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "336850901a214d11a7c0d9ed1859046e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5e558b632efd4125b60b32cbbfe538a6",
            "_dom_classes": [],
            "description": "Iteration: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bb6fe51455d4472d8d351d3766c831ca"
          }
        },
        "06c3e6a3725a479082db75c038bcef53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_98373a9762a04ec8bc15095ef5873d0a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 586/? [04:10&lt;00:00,  2.93it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41bdfa025d5142aa9a9c75422976e527"
          }
        },
        "5e558b632efd4125b60b32cbbfe538a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bb6fe51455d4472d8d351d3766c831ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98373a9762a04ec8bc15095ef5873d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41bdfa025d5142aa9a9c75422976e527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d6309d876c243f1b15fc0a78e621429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b0d3ed575602463c9ad52e4f5bf7f2e6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_04b0b339253f49a29fa540e8cad87a44",
              "IPY_MODEL_490e7763affe4b1eb90e08958ba25dfe"
            ]
          }
        },
        "b0d3ed575602463c9ad52e4f5bf7f2e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04b0b339253f49a29fa540e8cad87a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4817d2936a9f4986bb00b3d9d681c30b",
            "_dom_classes": [],
            "description": "Iteration: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d38f083894ab48f0ab1fca3bcc0fd43e"
          }
        },
        "490e7763affe4b1eb90e08958ba25dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_efdcfe5bf1834e6186eb744dfb59677b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 585/? [03:33&lt;00:00,  2.96it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f02f208814044bd9acb17e1de1c4420e"
          }
        },
        "4817d2936a9f4986bb00b3d9d681c30b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d38f083894ab48f0ab1fca3bcc0fd43e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "efdcfe5bf1834e6186eb744dfb59677b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f02f208814044bd9acb17e1de1c4420e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7YiMBVFIsO9",
        "colab_type": "text"
      },
      "source": [
        "# (TPU) Fine-tuning KB-ALBERT with `Transformers` TensorFlow ver.\n",
        "\n",
        "Hugging Face의 Transformers는 미세조정(fine-tuning)을 위해 필요한 API들을 간단하게 사용할 수 있는 형태로 제공합니다.\n",
        "기초적인 파이썬 프로그래밍과 기계학습 모델의 학습 방법만 알고있다면, 자연어처리(NLP)를 전공하지 않은 일반 사용자도 쉽게 최신 인공지능 언어모델을 불러와 사용할 수 있습니다.\n",
        "\n",
        "그리고 특정 목적을 위해 미세조정된 기계학습 모델을 새로운 데이터에서 쉽게 예측해 볼 수 있도록 Inference Pipeline도 API로 제공하고 있습니다. 그래서 누구나 쉽게 자신이 학습한 모델을 PyTorch나 TensorFlow의 naive programming을 하지 않아도 쉽게 모델을 학습하고 학습된 모델을 배포할 수 있습니다.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "## Objective\n",
        "- Hugging Face's Transformers를 활용한 fine-tuning\n",
        "- TPU를 사용하여 fine-tuning\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 네이버 영화리뷰 감성분석 (Naver Movie Review Sentiment Analysis)\n",
        "\n",
        "Naver sentiment movie corpus([link](https://github.com/e9t/nsmc))는 한국어 영화 리뷰 데이터로 리뷰 내용의 긍정과 부정이 라벨링 된 데이터입니다. 15만 개의 학습 데이터와 5만 개의 테스트 데이터로 나누어져 있습니다. 140개 이하의 짧은 문장으로 되어 있고, 쉽게 접근하여 사용해볼 수 있는 데이터입니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "## Contents\n",
        "\n",
        "1. 데이터 준비 & 필요 소스코드 다운로드\n",
        "2. Tokenizer를 활용한 학습용 데이터셋 생성\n",
        "3. 학습 하이퍼파라미터 설정\n",
        "4. Head를 활용한 모델 Fine-tuning\n",
        "5. Inference를 위한 Pipeline 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkm4zN7JIwO0",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 1. 데이터 및 필요 소스코드 등 준비\n",
        "\n",
        "예제를 수행하기 위해 필요한 데이터와 소스코드를 다운로드 해야 합니다.\n",
        "그리고 아쉽게도 KB-ALBERT는 내부 라이선스 이슈로 Transformers Model Hub에 업로드되어 있지 않아 별도의 과정을 통해 요청하신 후에 다운로드 받아 사용할 수 있습니다. 요청 방법은 [링크](https://github.com/KB-BANK-AI/KB-ALBERT-KO/kb-albert-char)를 참고해주세요.\n",
        "\n",
        "> 참고: 본 예제는 음절단위 모델을 기준으로 작성되었습니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "실행 내용\n",
        "1. Download source code for Custom Tokenizer\n",
        "2. Install Transformers library\n",
        "3. Download NSMC dataset\n",
        "\n",
        "**원활한 모델 학습을 위해 GPU 환경에서 테스트하시기를 권장드립니다.<br> 위 메뉴창에서 \"런타임\" > \"런타임 유형변경\" > \"하드웨어가속기 <font color='red'>TPU 선택</font>\" > \"저장\" 으로 환경을 변경하실 수 있습니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUDEH8gHCbdi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "180c7935-d7ce-4677-b7ac-8968d8d23302"
      },
      "source": [
        "# Download source codes\n",
        "!git clone https://github.com/sackoh/pycon-korea-2020-kb-albert.git\n",
        "%cd pycon-korea-2020-kb-albert/\n",
        "\n",
        "# Install transformers\n",
        "%pip install -q transformers\n",
        "\n",
        "# Download NSMC dataset\n",
        "!wget -q https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "!wget -q https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pycon-korea-2020-kb-albert'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 7 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (7/7), done.\n",
            "/content/pycon-korea-2020-kb-albert\n",
            "\u001b[K     |████████████████████████████████| 890kB 3.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 17.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 35.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.0MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OKYWmqFJBH-",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Upload pretrained model and configuration files\n",
        "\n",
        "다운로드 받은 파일을 **Google Colab**에서 사용하기 위해서는 파일 업로드가 필요합니다. 아래의 코드를 실행해주세요. 아래를 실행하면 파일 업로드를 위한 팝업창이 열립니다. 다음의 4가지 파일들을 *반드시* 선택하여 업로드를 진행해주세요.\n",
        "\n",
        "- config.json\n",
        "- pytorch_model.bin\n",
        "- tokenizer_config.json\n",
        "- vocab.txt\n",
        "\n",
        "업로드에 다소 시간이 걸릴 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir68peUCCgRn",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "1f306e15-74fd-439a-fd41-63faa28df3a0"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "%mkdir kb-albert-char\n",
        "%mv config.json pytorch_model.bin tokenizer_config.json vocab.txt ./kb-albert-char"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-be03f7e7-007b-47f3-87bc-f95b2e73fe78\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-be03f7e7-007b-47f3-87bc-f95b2e73fe78\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving config.json to config.json\n",
            "Saving pytorch_model.bin to pytorch_model.bin\n",
            "Saving tokenizer_config.json to tokenizer_config.json\n",
            "Saving vocab.txt to vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3NPlFcrI-M7",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Load train & test data\n",
        "\n",
        "NSMC 데이터를 불러옵니다. train data와 test data는 각각 리뷰 텍스트와 리뷰의 긍부정 라벨로 구성되어 있습니다.\n",
        "\n",
        "| text | label |\n",
        "| ---  | ---   |\n",
        "| 아 더빙.. 진짜 짜증나네요 목소리 | 0 |\n",
        "| 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나 | 1 |\n",
        "| 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ... | 1 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NAJ3F4HCjP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "def read_nsmc_data(file_path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as r:\n",
        "        reader = csv.reader(r, delimiter=\"\\t\")\n",
        "        next(reader, None)\n",
        "        for line in reader:\n",
        "            texts.append(line[1])\n",
        "            labels.append(int(line[2]))\n",
        "    return texts, labels\n",
        "\n",
        "data_dir = Path('./')\n",
        "train_texts, train_labels = read_nsmc_data(data_dir/'ratings_train.txt')\n",
        "test_texts, test_labels = read_nsmc_data(data_dir/'ratings_test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4eEefxiJ1Wc",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 2. 학습 하이퍼파라미터 설정\n",
        "\n",
        "`Transformers`에서 제공하는 `TFTrainingArguments`에 학습 하이퍼파라미터를 설정해줍니다.\n",
        "\n",
        "설정하지 않은 하이퍼파라미터들은 default 값을 사용합니다.\n",
        "\n",
        "하이퍼파라미터 설정을 위한 상세 내용은 [링크](https://huggingface.co/transformers/main_classes/trainer.html?highlight=trainingargument#transformers.TFTrainingArguments)를 참조해주시기 바랍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RZ9wE_GVv3Z",
        "colab_type": "text"
      },
      "source": [
        "## GCP Bucket 사용을 위해 GCP project 권한 연동\n",
        "\n",
        "<font color='#880049'>TPU를 사용하기 위해서는 GCP Storage가 있어야 합니다.\n",
        "Google Cloud에서 Storage를 생성한 후에 다음의 작업을 진행해야 합니다.\n",
        "관련된 내용은 [link](https://medium.com/fenwicks/tutorial-0-setting-up-google-colab-tpu-runtime-and-cloud-storage-b88d34aa9dcb) 를 참조해주세요.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QavH_gFCjrRU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5d68453f-91d0-4dc5-82ae-baccc47bd5b0"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!gcloud config set project {GCP_PROJECT}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW8-RXZqV749",
        "colab_type": "text"
      },
      "source": [
        "<font color='#880049'>TPU를 위해서는 fine-tuning 모델이 생성되는 경로와 log가 생성되는 경로가 GCP bucket 경로이어야 합니다.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxImA32QKA6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import TFTrainingArguments\n",
        "\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir = \"gs://{GCP_PROJECT}/{GCP_BUCKET}/\" #@param {type:\"string\"}\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir = \"gs://{GCP_PROJECT}/{GCP_BUCKET}/logs\" #@param {type:\"string\"}\n",
        "    logging_steps=2000\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM0UiXHZJLcQ",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 3. Tokenizer를 활용한 학습용 데이터셋 생성\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDhtWAG1JPgM",
        "colab_type": "text"
      },
      "source": [
        "## Custom Tokenizer 불러오기\n",
        "\n",
        "본 예제는 음절단위(Character-level) 모델을 사용하기 때문에 한국어를 위한 음절단위 Tokenizer를 사용해야 합니다. `Transformers`에서 공식 Tokenizer로 등록이 안됐기 때문에 이전 단계에서 다운로드한 소스코드에서 Custom Tokenizer를 불러옵니다.\n",
        "\n",
        "만약 공식 Hub에 등록된 Bert multilingual 토크나이저를 사용한다면 다음과 같이 불러오면 됩니다.\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilDuSiIWCpdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tokenization_kbalbert import KbAlbertCharTokenizer\n",
        "\n",
        "tokenizer = KbAlbertCharTokenizer.from_pretrained('kb-albert-char')\n",
        "\n",
        "# from transformers import BertTokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfvQnniiJUX-",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Tokenizer를 통해 Raw text를 전처리\n",
        "\n",
        "불러온 tokenizer를 활용해 input 데이터를 전처리합니다. 텍스트를 sparse indices 형태로 변환하는 것이 주요 처리 내용입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akXuCfPyCp-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw3Po4HyJYzi",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Tensorflow Dataset으로 학습용 데이터셋 생성\n",
        "\n",
        "전처리된 데이터를 텐서플로의의 `Dataset` 안의 `from_tensor_slice`로 만들어줍니다. `from_tensor_slices`와 관련된 자세한 내용은 [링크](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices)를 참조해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lBrAO3WCt3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "   dict(train_encodings),\n",
        "   train_labels\n",
        "))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GleL38MeV-wH",
        "colab_type": "text"
      },
      "source": [
        "<font color='#880049'> TPU는 ditributed 환경을 사용하기 때문에 tensorflow tensor dataset을 그에 맞게 만들어줍니다. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnmHJWb7XezS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def get_tfdataset(dataset, args):\n",
        "    num_examples = tf.data.experimental.cardinality(dataset).numpy()\n",
        "\n",
        "    if num_examples < 0:\n",
        "        raise ValueError(\"The training dataset must have an asserted cardinality\")\n",
        "\n",
        "    approx = math.floor if args.dataloader_drop_last else math.ceil\n",
        "    steps = approx(num_examples / args.eval_batch_size)\n",
        "    ds = (\n",
        "        test_dataset.repeat()\n",
        "        .batch(args.eval_batch_size, drop_remainder=args.dataloader_drop_last)\n",
        "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    return args.strategy.experimental_distribute_dataset(ds), steps, num_examples\n",
        "\n",
        "\n",
        "train_ds, _, train_num_examples = get_tfdataset(train_dataset, training_args)\n",
        "test_ds, test_steps, test_num_examples = get_tfdataset(test_dataset, training_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06ra-fSBKP6Y",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 4. Head를 활용한 모델 Fine-tuning\n",
        "\n",
        "네이버 영화리뷰 감성분석을 위해 텍스트(sequence of words)에서 긍부정(1/0)을 예측하는 모델을 학습합니다. Sequence의 분류문제(Classification) 미세조정을 위한 Head인 `XXXForSequenceClassification`을 불러옵니다. 본 예제는 ALBERT 언어모델을 사용하고 텐서플로를 활용할 예정이기 때문에 `TFAlbertForSequenceClassification` Head class를 불러옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STN9TZDdKpYW",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning을 위한 Head에 사전학습한 언어모델 불러오기\n",
        "\n",
        "Head의 `from_pretrained` 메소드를 통해 사전학습한 KB-ALBERT 언어모델을 함께 불러옵니다. 이때 인자값으로 모델이 위치한 디렉토리(폴더) 경로를 넘겨줍니다.\n",
        "\n",
        "> Head는 Pretrained Language Model과 Output Layer로 구성되어 있습니다.\n",
        "\n",
        "미세조정을 통해 Language Model과 Output Layer의 weight가 함께 조정됩니다.\n",
        "\n",
        "사전학습된 언어모델은 pytorch 모델이기 때문에 `from_pretrained`로 불러올 때 `from_pt=True`로 설정해주어어야 합니다.\n",
        "\n",
        "BERT multilingual 모델 사용을 위해서는 다음과 같입 입력해주어야 합니다.\n",
        "```python\n",
        "from transformers import TFBertForSequenceClassification\n",
        "\n",
        "with training_args.strategy.scope():\n",
        "    model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OP8Z62kCwDO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "deb1a1bf-3ef6-4295-f35e-60ce69eb2c78"
      },
      "source": [
        "from transformers import TFAlbertForSequenceClassification\n",
        "\n",
        "with training_args.strategy.scope():\n",
        "    model = TFAlbertForSequenceClassification.from_pretrained('kb-albert-char', from_pt=True)\n",
        "\n",
        "\n",
        "# from transformers import TFBertForSequenceClassification\n",
        "# with training_args.strategy.scope():\n",
        "#     model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertForSequenceClassification: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing TFAlbertForSequenceClassification from a TF 2.0 model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a TFBertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from a TF 2.0 model that you expect to be exactly identical (e.g. initializing a BertForSequenceClassification model from a TFBertForSequenceClassification model).\n",
            "Some weights or buffers of the PyTorch model TFAlbertForSequenceClassification were not initialized from the TF 2.0 model and are newly initialized: ['sop_classifier.classifier.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'sop_classifier.classifier.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hjU4CprK3Oz",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## Trainer를 통해 fine-tuning 수행\n",
        "\n",
        "그 다음은 학습을 위해 `Trainer` class를 불러옵니다. Head와 학습 하이퍼파라미터 그리고 앞에서 생성한 학습용 데이터셋을 인자로 넘긴 후에 `train()` 메소드로 학습을 시작합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ9wAfsVg1pE",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Customized Trainer for TPU (Shift + Enter)\n",
        "import os\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from transformers import TFTrainer\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, EvalPrediction, PredictionOutput, set_seed\n",
        "from transformers.utils import logging\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "class TFTrainer_(TFTrainer):\n",
        "\n",
        "    def train(self) -> None:\n",
        "        \"\"\"\n",
        "        Train method to train the model.\n",
        "        \"\"\"\n",
        "        train_ds = self.train_dataset\n",
        "        self.total_train_batch_size = self.args.train_batch_size * self.args.gradient_accumulation_steps\n",
        "        self.num_train_examples = 150000\n",
        "\n",
        "        if self.args.debug:\n",
        "            tf.summary.trace_on(graph=True, profiler=True)\n",
        "\n",
        "        self.gradient_accumulator.reset()\n",
        "\n",
        "        if self.args.max_steps > 0:\n",
        "            t_total = self.args.max_steps\n",
        "            self.steps_per_epoch = self.args.max_steps\n",
        "        else:\n",
        "            approx = math.floor if self.args.dataloader_drop_last else math.ceil\n",
        "            self.steps_per_epoch = approx(self.num_train_examples / self.total_train_batch_size)\n",
        "            t_total = self.steps_per_epoch * self.args.num_train_epochs\n",
        "\n",
        "        with self.args.strategy.scope():\n",
        "            self.create_optimizer_and_scheduler(num_training_steps=t_total)\n",
        "            iterations = self.optimizer.iterations\n",
        "            self.global_step = iterations.numpy()\n",
        "            folder = os.path.join(self.args.output_dir, PREFIX_CHECKPOINT_DIR)\n",
        "            ckpt = tf.train.Checkpoint(optimizer=self.optimizer, model=self.model)\n",
        "            self.model.ckpt_manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=self.args.save_total_limit)\n",
        "\n",
        "            if self.model.ckpt_manager.latest_checkpoint:\n",
        "                epochs_trained = self.global_step // (self.num_train_examples // self.args.gradient_accumulation_steps)\n",
        "                steps_trained_in_current_epoch = self.global_step % (\n",
        "                    self.num_train_examples // self.args.gradient_accumulation_steps\n",
        "                )\n",
        "\n",
        "                logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "                logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "                logger.info(\"  Continuing training from global step %d\", self.global_step)\n",
        "                logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "                logger.info(\n",
        "                    \"Checkpoint file %s found and restoring from checkpoint\", self.model.ckpt_manager.latest_checkpoint\n",
        "                )\n",
        "\n",
        "                ckpt.restore(self.model.ckpt_manager.latest_checkpoint).expect_partial()\n",
        "            else:\n",
        "                epochs_trained = 1\n",
        "\n",
        "            tf.summary.experimental.set_step(iterations)\n",
        "\n",
        "            epochs = 1 if self.args.max_steps > 0 else self.args.num_train_epochs\n",
        "\n",
        "            if self.args.fp16:\n",
        "                policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
        "                tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "\n",
        "            with self.tb_writer.as_default():\n",
        "                tf.summary.text(\"args\", self.args.to_json_string())\n",
        "\n",
        "            self.tb_writer.flush()\n",
        "\n",
        "            logger.info(\"***** Running training *****\")\n",
        "            logger.info(\"  Num examples = %d\", self.num_train_examples)\n",
        "            logger.info(\"  Num Epochs = %d\", epochs)\n",
        "            logger.info(\"  Instantaneous batch size per device = %d\", self.args.per_device_train_batch_size)\n",
        "            logger.info(\n",
        "                \"  Total train batch size (w. parallel, distributed & accumulation) = %d\", self.total_train_batch_size\n",
        "            )\n",
        "            logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n",
        "            logger.info(\"  Steps per epoch = %d\", self.steps_per_epoch)\n",
        "            logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "            self.train_loss = tf.keras.metrics.Sum()\n",
        "            start_time = datetime.datetime.now()\n",
        "\n",
        "            train_iterator = tqdm(range(epochs_trained, int(epochs + 1)), desc=\"Epoch\")\n",
        "            for epoch_iter in train_iterator:\n",
        "                # Reset the past mems state at the beginning of each epoch if necessary.\n",
        "                if self.args.past_index >= 0:\n",
        "                    self._past = None\n",
        "\n",
        "                epoch_iterator = tqdm(train_ds, desc=\"Iteration\")\n",
        "                for step, batch in enumerate(epoch_iterator):\n",
        "                    self.global_step = iterations.numpy()\n",
        "                    self.epoch_logging = epoch_iter - 1 + (step + 1) / self.steps_per_epoch\n",
        "\n",
        "                    self.distributed_training_steps(batch)\n",
        "\n",
        "                    training_loss = self.train_loss.result() / ((step + 1) * self.total_train_batch_size)\n",
        "\n",
        "                    if self.args.debug:\n",
        "                        logs = {}\n",
        "                        logs[\"loss\"] = training_loss.numpy()\n",
        "                        logs[\"epoch\"] = self.epoch_logging\n",
        "\n",
        "                        self.log(logs)\n",
        "\n",
        "                    if self.global_step == 1 and self.args.debug:\n",
        "                        with self.tb_writer.as_default():\n",
        "                            tf.summary.trace_export(\n",
        "                                name=\"training\", step=self.global_step, profiler_outdir=self.args.logging_dir\n",
        "                            )\n",
        "\n",
        "                    if (\n",
        "                        self.global_step > 0\n",
        "                        and self.args.evaluate_during_training\n",
        "                        and self.global_step % self.args.eval_steps == 0\n",
        "                    ):\n",
        "                        self.evaluate()\n",
        "\n",
        "                    if (self.global_step > 0 and self.global_step % self.args.logging_steps == 0) or (\n",
        "                        self.global_step == 1 and self.args.logging_first_step\n",
        "                    ):\n",
        "                        logs = {}\n",
        "                        logs[\"loss\"] = training_loss.numpy()\n",
        "                        logs[\"learning_rate\"] = self.lr_scheduler(self.global_step).numpy()\n",
        "                        logs[\"epoch\"] = self.epoch_logging\n",
        "\n",
        "                        self.log(logs)\n",
        "\n",
        "                    if self.global_step > 0 and self.global_step % self.args.save_steps == 0:\n",
        "                        ckpt_save_path = self.model.ckpt_manager.save()\n",
        "\n",
        "                        logger.info(\"Saving checkpoint for step {} at {}\".format(self.global_step, ckpt_save_path))\n",
        "\n",
        "                    if self.global_step > 0 and self.global_step % self.steps_per_epoch == 0:\n",
        "                        break\n",
        "\n",
        "                self.train_loss.reset_states()\n",
        "\n",
        "            end_time = datetime.datetime.now()\n",
        "\n",
        "            logger.info(\"Training took: {}\".format(str(end_time - start_time)))\n",
        "\n",
        "        if self.args.past_index and hasattr(self, \"_past\"):\n",
        "            # Clean the state at the end of training\n",
        "            delattr(self, \"_past\")\n",
        "\n",
        "    def evaluate(self, eval_ds, steps, num_examples):\n",
        "        \"\"\"\n",
        "        Run evaluation and returns metrics.\n",
        "        The calling script will be responsible for providing a method to compute metrics, as they are\n",
        "        task-dependent (pass it to the init :obj:`compute_metrics` argument).\n",
        "        Args:\n",
        "            eval_dataset (:class:`~tf.data.Dataset`, `optional`):\n",
        "                Pass a dataset if you wish to override :obj:`self.eval_dataset`.\n",
        "        Returns:\n",
        "            A dictionary containing the evaluation loss and the potential metrics computed from the predictions.\n",
        "        \"\"\"\n",
        "        print(\"Evaluating...\")\n",
        "\n",
        "        with self.args.strategy.scope():\n",
        "            output = self.prediction_loop(eval_ds, steps, num_examples, description=\"Evaluation\")\n",
        "            logs = {**output.metrics}\n",
        "            logs[\"epoch\"] = self.epoch_logging\n",
        "\n",
        "            self.log(logs)\n",
        "            print(output.metrics)\n",
        "\n",
        "        return output.metrics\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_prediction_steps(self, batch):\n",
        "        with self.args.strategy.scope():\n",
        "            logits = self.args.strategy.run(self.prediction_step, batch)\n",
        "\n",
        "        return logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWU2DY9MK26v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252,
          "referenced_widgets": [
            "87731483deed4002bc48cd3e3565f542",
            "b9178d1919be4bf4b6491d361e8fee14",
            "2ad038ca23754fb492707b82e483aa9a",
            "4006510ea3e84015bc9352a4e9ebf4a1",
            "8bc7710c270541ad92efbc42c154517e",
            "704a6ef7cd3b44329af7a8f6fce4c407",
            "16e4756ea567472ba427cca12d7657d7",
            "3560cb73a7e8456c8477c86137cdf354",
            "afe6c2729a354c639108c5c92ee61438",
            "be52f7c4c8574e99aea19e845959a590",
            "336850901a214d11a7c0d9ed1859046e",
            "06c3e6a3725a479082db75c038bcef53",
            "5e558b632efd4125b60b32cbbfe538a6",
            "bb6fe51455d4472d8d351d3766c831ca",
            "98373a9762a04ec8bc15095ef5873d0a",
            "41bdfa025d5142aa9a9c75422976e527",
            "5d6309d876c243f1b15fc0a78e621429",
            "b0d3ed575602463c9ad52e4f5bf7f2e6",
            "04b0b339253f49a29fa540e8cad87a44",
            "490e7763affe4b1eb90e08958ba25dfe",
            "4817d2936a9f4986bb00b3d9d681c30b",
            "d38f083894ab48f0ab1fca3bcc0fd43e",
            "efdcfe5bf1834e6186eb744dfb59677b",
            "f02f208814044bd9acb17e1de1c4420e"
          ]
        },
        "outputId": "d920de7a-2ad9-452b-98ae-3145f1f4b056"
      },
      "source": [
        "trainer = TFTrainer_(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87731483deed4002bc48cd3e3565f542",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afe6c2729a354c639108c5c92ee61438",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Iteration', max=1.0, style=ProgressStyl…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d6309d876c243f1b15fc0a78e621429",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Iteration', max=1.0, style=ProgressStyl…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmHcYtKtVC3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = TFTrainer_(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tep1Ml-5LGUB",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# 5. Inference를 위한 Pipeline 생성\n",
        "\n",
        "`Transformers`는 미세조정된 모델에 raw text를 인풋으로 넣었을 때 데이터 전처리와 모델 추론 과정을 `pipeline` class를 통해 api 형태로 쉽게 개발할 수 있도록 하였습니다.\n",
        "\n",
        "pipeline에는 사전 정의된 task 유형과 미세조정된 모델, tokenizer 그리고 딥러닝 프레임워크 유형을 인자로 전달합니다. (PyTorch는 'pt', TensorFlow는 'tf')\n",
        "\n",
        "생성된 pipeline 인스턴스에 raw text를 넣어주면 예측 결과와 예측 결과에 대한 confidence 값이 반환됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGus2qHAUTxr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "713c9fdd-b50d-4de4-fe12-48c74323f0b6"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nsmc_classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, framework='tf')\n",
        "id2label = {\"LABEL_0\": \"negative\", \"LABEL_1\": \"positive\"}\n",
        "\n",
        "reviews= [\n",
        "          \"생각한 것보다는 영화가 재미없었어\",\n",
        "          \"OO!\",\n",
        "          \"역시 재밌네ㅋㅋ 최고\"\n",
        "]\n",
        "\n",
        "results = nsmc_classifier(reviews)\n",
        "for idx, result in enumerate(results):\n",
        "    print(reviews[idx])\n",
        "    for k, v in result.items():\n",
        "        print(f\" >> {k} : {id2label[v] if k == 'label' else v}\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "생각한 것보다는 영화가 재미없었어\n",
            " >> label : negative\n",
            " >> score : 0.998558521270752\n",
            "\n",
            "OO!\n",
            " >> label : negative\n",
            " >> score : 0.9973821640014648\n",
            "\n",
            "역시 재밌네ㅋㅋ 최고\n",
            " >> label : positive\n",
            " >> score : 0.9665402770042419\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRriRPhELPYM",
        "colab_type": "text"
      },
      "source": [
        "pipeline이나 미세조정된 모델은 `save_pretrained`를 통해 지정된 경로에 저장할 수 있습니다. 다시 사용해야 할 때는 같은 경로명을 `from_pretrained`에 넣어주어 쉽게 불러올 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpm8psakVPv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nsmc_classifier.save_pretrained('./pipeline')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jDy-f20UhXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}